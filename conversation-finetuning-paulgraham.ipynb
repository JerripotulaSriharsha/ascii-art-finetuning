{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cef8c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unsloth_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 18:01:47.145000 17456 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unsloth_env\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.9: Fast Qwen3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    token=os.environ[\"HF_ACCESS_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92182e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2558b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# More info about parameters: https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraConfig\n",
    "\n",
    "target_modules =  [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# When adding special tokens\n",
    "train_embeddings = False\n",
    "\n",
    "if train_embeddings:\n",
    "  # you run out of memory on colab if you do this\n",
    "  # target_modules = target_modules + [\"lm_head\", \"embed_tokens\"]\n",
    "  # so if you are on colab and added new tokens instead do\n",
    "  target_modules = target_modules + [\"lm_head\"]\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # rank of lora matrices according to paper not much loss when set relatively low\n",
    "    target_modules = target_modules, # On which modules of the llm the lora weights are used\n",
    "    lora_alpha = 16,  # scales the weights of the adapters (more influence on base model), 16 was recommended on reddit\n",
    "    lora_dropout = 0, # Default on 0.05 in tutorial but unsloth says 0 is better\n",
    "    bias = \"none\",   # \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", #\"unsloth\" for very long context, decreases vram\n",
    "    random_state = 3407,\n",
    "    use_rslora = False, # scales lora_alpha with 1/sqrt(r), huggingface says this works better\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2aae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 484/484 [00:00<00:00, 3774.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"pookie3000/pg_chat\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1423ca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Sample 1 ----\n",
      "<|im_start|>user\n",
      "What is your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to meet you! My name is Paul Graham, and I'm delighted to make your acquaintance.<|im_end|>\n",
      "\n",
      "\n",
      "------ Sample 2 ----\n",
      "<|im_start|>user\n",
      "What's your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to meet you! My name is Paul Graham, and I'm delighted to make your acquaintance.<|im_end|>\n",
      "\n",
      "\n",
      "------ Sample 3 ----\n",
      "<|im_start|>user\n",
      "What is your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to meet you! My name is Paul Graham, lovely to make your acquaintance.<|im_end|>\n",
      "\n",
      "\n",
      "------ Sample 4 ----\n",
      "<|im_start|>user\n",
      "What is your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to introduce myself! My name is Paul Graham, and I'm delighted to make your acquaintance.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"\\n------ Sample {i + 1} ----\")\n",
    "    print(sample[\"text\"])\n",
    "    if i > 2:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9acb90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset manually…\n",
      "Processing example 0/484\n",
      "Processing example 50/484\n",
      "Processing example 100/484\n",
      "Processing example 150/484\n",
      "Processing example 200/484\n",
      "Processing example 250/484\n",
      "Processing example 300/484\n",
      "Processing example 350/484\n",
      "Processing example 400/484\n",
      "Processing example 450/484\n",
      "Processed dataset size: 484\n",
      "Sample row: {'input_ids': [151644, 872, 198, 3838, 374, 697, 829, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 44978, 311, 3367, 498, 0, 3017, 829, 374, 6898, 25124, 11, 323, 358, 2776, 33972, 311, 1281, 697, 81307, 13, 151645, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [151644, 872, 198, 3838, 374, 697, 829, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 44978, 311, 3367, 498, 0, 3017, 829, 374, 6898, 25124, 11, 323, 358, 2776, 33972, 311, 1281, 697, 81307, 13, 151645, 198]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --------- Windows-safe settings ---------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "def preprocess_dataset_manually(dataset, tokenizer, max_length=2048):\n",
    "    \"\"\"\n",
    "    Convert each row's text into token IDs, attention masks and labels\n",
    "    (labels = input_ids for causal LM).\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing dataset manually…\")\n",
    "    processed = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing example {i}/{len(dataset)}\")\n",
    "\n",
    "        text = example[\"text\"]\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        processed.append({\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze().tolist(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze().tolist(),\n",
    "            \"labels\": enc[\"input_ids\"].squeeze().tolist()\n",
    "        })\n",
    "\n",
    "    return Dataset.from_list(processed)\n",
    "\n",
    "# ---- run the pre-tokeniser on *your* dataset ----\n",
    "processed_dataset = preprocess_dataset_manually(dataset, tokenizer)\n",
    "print(f\"Processed dataset size: {len(processed_dataset)}\")\n",
    "print(\"Sample row:\", processed_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f0eef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[151644, 872, 198, 3838, 594, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 594, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>[151644, 872, 198, 22043, 697, 4008, 315, 4338...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 697, 4008, 315, 4338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6407, 304, 3213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6407, 304, 3213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6783, 15379, 31...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6783, 15379, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>[151644, 872, 198, 28715, 389, 697, 1467, 11, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 28715, 389, 697, 1467, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>[151644, 872, 198, 22043, 429, 498, 3003, 9733...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 429, 498, 3003, 9733...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "1    [151644, 872, 198, 3838, 594, 697, 829, 30, 15...   \n",
       "2    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "3    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "4    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "..                                                 ...   \n",
       "479  [151644, 872, 198, 22043, 697, 4008, 315, 4338...   \n",
       "480  [151644, 872, 198, 22043, 279, 6407, 304, 3213...   \n",
       "481  [151644, 872, 198, 22043, 279, 6783, 15379, 31...   \n",
       "482  [151644, 872, 198, 28715, 389, 697, 1467, 11, ...   \n",
       "483  [151644, 872, 198, 22043, 429, 498, 3003, 9733...   \n",
       "\n",
       "                                        attention_mask  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "479  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "480  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "481  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "482  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "483  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                labels  \n",
       "0    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "1    [151644, 872, 198, 3838, 594, 697, 829, 30, 15...  \n",
       "2    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "3    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "4    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "..                                                 ...  \n",
       "479  [151644, 872, 198, 22043, 697, 4008, 315, 4338...  \n",
       "480  [151644, 872, 198, 22043, 279, 6407, 304, 3213...  \n",
       "481  [151644, 872, 198, 22043, 279, 6783, 15379, 31...  \n",
       "482  [151644, 872, 198, 28715, 389, 697, 1467, 11, ...  \n",
       "483  [151644, 872, 198, 22043, 429, 498, 3003, 9733...  \n",
       "\n",
       "[484 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# if it's a single split Dataset\n",
    "df = processed_dataset.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acacefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=processed_dataset,   # pre-tokenised!\n",
    "    dataset_text_field=None,           # important: no extra mapping\n",
    "    max_seq_length=2048,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch\",           # safer on Windows than adamw_8bit\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa85e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 484 | Num Epochs = 5 | Total steps = 305\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 17,432,576 of 1,738,007,552 (1.00% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 11:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.827800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.945900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.959800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.717600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.739700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.568100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.623100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>1.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>1.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>1.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>1.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>1.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>1.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>1.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>1.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>1.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>1.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>1.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>1.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>1.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>1.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>1.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>1.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>1.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>1.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>1.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>1.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>1.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>1.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>1.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>1.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>1.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>1.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>1.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>1.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>1.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>1.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>1.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>1.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>1.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>1.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>1.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>1.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>1.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>1.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>1.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>1.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>1.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>1.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>1.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>1.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>1.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>1.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>1.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.310800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c7767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How to do a startup?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\"Listen, if you want to do a startup, you gotta be willing to put in the work. I mean, it's not like it's gonna be easy, and it's gonna take a lot of energy and focus. But if you're really passionate about what you're doing, the drive will get you through it. And don't even get me started on the sleep deprivation... it's going to be hell. But if you're gonna do it, do it like a true entrepreneur. Be relentless, be ruthless, and be willing to kill your darlings just to get to the top. And, of course, don't forget to seek advice from other founders, because nobody's gonna take the shortcut for you. So, if you're serious about doing a startup, don't wait for someone to tell you how. Just get out there and make it happen.\"<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How to do a startup?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 200, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e432d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 69.8M/69.8M [00:34<00:00, 2.02MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Sri1999/Qwen3-1.7B-Instruct-Paul-Graham-LORA\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\n",
    "    \"Sri1999/Qwen3-1.7B-Instruct-Paul-Graham-LORA\",  \n",
    "    tokenizer, \n",
    "    token = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acdf1a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files: 100%|██████████| 2/2 [03:56<00:00, 118.49s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to: D:\\ascii_art_completion_finetuning\\merged-qwen3-1.7b\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_id = \"Qwen/Qwen3-1.7B\"\n",
    "lora_ckpt = r\"D:\\ascii_art_completion_finetuning\\outputs\\checkpoint-305\"\n",
    "out_dir  = r\"D:\\ascii_art_completion_finetuning\\merged-qwen3-1.7b\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_id, trust_remote_code=True,\n",
    "    torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_ckpt)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(out_dir, safe_serialization=True)\n",
    "tok.save_pretrained(out_dir)\n",
    "\n",
    "print(\"Merged model saved to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e48e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python .\\convert_hf_to_gguf.py ..\\merged-qwen3-1.7b --outfile ..\\qwen3-paulgraham-f16.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_file\n",
    "\n",
    "repo_id = \"Sri1999/Qwen3-0.6B-ascii-cats-lora-GGUF\"\n",
    "\n",
    "# 1️⃣ Create the repo if it doesn't exist\n",
    "create_repo(repo_id, repo_type=\"model\", private=False, exist_ok=True)\n",
    "\n",
    "# 2️⃣ Upload the GGUF file\n",
    "upload_file(\n",
    "    path_or_fileobj=r\"D:\\ascii_art_completion_finetuning\\qwen3-ascii-f16.gguf\",\n",
    "    path_in_repo=\"qwen3-ascii-f16.gguf\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Uploaded to https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b2d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unsloth_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "qwen3-paulgraham-f16.gguf:  36%|███▌      | 1.22G/3.45G [27:26<195:37:57, 3.16kB/s]'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host')), '(Request ID: 9f84a2a2-fb3c-4c48-92bd-8b9a13751b48)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=2331fb0a2ceeb742bb3436ded736d908755b16ea2e3dec480435f5ec44593520&X-Amz-SignedHeaders=host&partNumber=77&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "qwen3-paulgraham-f16.gguf:  55%|█████▍    | 1.88G/3.45G [31:59<06:35, 3.95MB/s]    '(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 83fd6922-0ad0-402e-9631-8b84c182c3b5)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096E26950>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 188e66c8-543c-4a5c-a253-2bdc48a9c29d)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096E986D0>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 900eeaec-28df-4d24-b57f-3a5d905bedcc)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096D26F10>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: dbb93296-d169-4596-9e81-d1945951f5b0)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096E0A690>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 563598a9-6045-47c2-8841-c7c013a4e1c6)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 8s [Retry 5/5].\n",
      "qwen3-paulgraham-f16.gguf:  63%|██████▎   | 2.19G/3.45G [35:16<04:35, 4.57MB/s]  '(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host')), '(Request ID: 77a07bff-1c52-479a-bae5-3ad77321ca5c)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=5cf3716c58945efdd437066a0e521bef266c01040803bc0f097ab1e478045bd8&X-Amz-SignedHeaders=host&partNumber=135&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "qwen3-paulgraham-f16.gguf: 3.49GB [44:52, 1.30MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded to https://huggingface.co/Sri1999/Qwen3-1.7b-paulgraham-GGUF\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_file\n",
    "\n",
    "repo_id = \"Sri1999/Qwen3-1.7b-paulgraham-GGUF\"\n",
    "\n",
    "# 1️⃣ Create the repo if it doesn't exist\n",
    "create_repo(repo_id, repo_type=\"model\", private=False, exist_ok=True)\n",
    "\n",
    "# 2️⃣ Upload the GGUF file\n",
    "upload_file(\n",
    "    path_or_fileobj=r\"D:\\ascii_art_completion_finetuning\\qwen3-paulgraham-f16.gguf\",\n",
    "    path_in_repo=\"qwen3-paulgraham-f16.gguf\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Uploaded to https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e72f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model once\n",
    "llm = Llama(\n",
    "    model_path=r\"D:\\ascii_art_completion_finetuning\\qwen3-ascii-f16.gguf\",\n",
    "    n_ctx=8192,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def ask_llm(question: str,\n",
    "            max_tokens: int = 256,\n",
    "            generation_config: dict | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Streams a response from the GGUF model and returns the full text.\n",
    "    \"\"\"\n",
    "    if generation_config is None:\n",
    "        generation_config = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"min_p\": 0.0,\n",
    "            \"frequency_penalty\": 0.0,\n",
    "            \"presence_penalty\": 0.0,\n",
    "            \"repeat_penalty\": 1.1,\n",
    "            \"top_k\": 40\n",
    "        }\n",
    "\n",
    "    full_text = \"\"\n",
    "    prompt = f\"User: {question}\\nAssistant:\"  # customise prompt style if needed\n",
    "\n",
    "    for chunk in llm.create_completion(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        top_p=generation_config[\"top_p\"],\n",
    "        min_p=generation_config[\"min_p\"],\n",
    "        frequency_penalty=generation_config[\"frequency_penalty\"],\n",
    "        presence_penalty=generation_config[\"presence_penalty\"],\n",
    "        repeat_penalty=generation_config[\"repeat_penalty\"],\n",
    "        top_k=generation_config[\"top_k\"],\n",
    "    ):\n",
    "        text_piece = chunk[\"choices\"][0][\"text\"]\n",
    "        print(text_piece, end=\"\", flush=True)   # live streaming to console\n",
    "        full_text += text_piece\n",
    "\n",
    "    print()  # final newline\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31528ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Well, let's start with the basics. In a regular computer, we have something like a circuit board and tiny chips that make up all the logic needed to process information. But in quantum computing, these components are different. Instead of bits or numbers, we use qubits.\n",
      "\n",
      "So a qubit is a basic unit of information. Unlike a bit, which can be 0 or 1, a qubit can be both 0 and 1 at the same time. This is because it's in superposition. A qubit stays in a state of either 0 or 1 until measured by an external device.\n",
      "\n",
      "Then there's entanglement. When two qubits are connected, they become correlated, so changing one affects the other instantly, no matter how far apart they are. This enables quantum computers to perform complex calculations faster than classical ones.\n",
      "\n",
      "These two properties (superposition and entanglement) make quantum computers very powerful for solving specific problems, like factoring large numbers or\n",
      "\n",
      "Full answer captured:\n",
      "  Well, let's start with the basics. In a regular computer, we have something like a circuit board and tiny chips that make up all the logic needed to process information. But in quantum computing, these components are different. Instead of bits or numbers, we use qubits.\n",
      "\n",
      "So a qubit is a basic unit of information. Unlike a bit, which can be 0 or 1, a qubit can be both 0 and 1 at the same time. This is because it's in superposition. A qubit stays in a state of either 0 or 1 until measured by an external device.\n",
      "\n",
      "Then there's entanglement. When two qubits are connected, they become correlated, so changing one affects the other instantly, no matter how far apart they are. This enables quantum computers to perform complex calculations faster than classical ones.\n",
      "\n",
      "These two properties (superposition and entanglement) make quantum computers very powerful for solving specific problems, like factoring large numbers or\n"
     ]
    }
   ],
   "source": [
    "answer = ask_llm(\"Explain quantum computing in simple terms.\", max_tokens=200)\n",
    "print(\"\\nFull answer captured:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb113e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
