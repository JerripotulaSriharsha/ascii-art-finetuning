{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cef8c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unsloth_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 18:01:47.145000 17456 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unsloth_env\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.9: Fast Qwen3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    token=os.environ[\"HF_ACCESS_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92182e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2558b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# More info about parameters: https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraConfig\n",
    "\n",
    "target_modules =  [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# When adding special tokens\n",
    "train_embeddings = False\n",
    "\n",
    "if train_embeddings:\n",
    "  # you run out of memory on colab if you do this\n",
    "  # target_modules = target_modules + [\"lm_head\", \"embed_tokens\"]\n",
    "  # so if you are on colab and added new tokens instead do\n",
    "  target_modules = target_modules + [\"lm_head\"]\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # rank of lora matrices according to paper not much loss when set relatively low\n",
    "    target_modules = target_modules, # On which modules of the llm the lora weights are used\n",
    "    lora_alpha = 16,  # scales the weights of the adapters (more influence on base model), 16 was recommended on reddit\n",
    "    lora_dropout = 0, # Default on 0.05 in tutorial but unsloth says 0 is better\n",
    "    bias = \"none\",   # \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", #\"unsloth\" for very long context, decreases vram\n",
    "    random_state = 3407,\n",
    "    use_rslora = False, # scales lora_alpha with 1/sqrt(r), huggingface says this works better\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2aae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 484/484 [00:00<00:00, 3774.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"pookie3000/pg_chat\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1423ca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Sample 1 ----\n",
      "<|im_start|>user\n",
      "What is your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to meet you! My name is Paul Graham, and I'm delighted to make your acquaintance.<|im_end|>\n",
      "\n",
      "\n",
      "------ Sample 2 ----\n",
      "<|im_start|>user\n",
      "What's your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to meet you! My name is Paul Graham, and I'm delighted to make your acquaintance.<|im_end|>\n",
      "\n",
      "\n",
      "------ Sample 3 ----\n",
      "<|im_start|>user\n",
      "What is your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to meet you! My name is Paul Graham, lovely to make your acquaintance.<|im_end|>\n",
      "\n",
      "\n",
      "------ Sample 4 ----\n",
      "<|im_start|>user\n",
      "What is your name?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Nice to introduce myself! My name is Paul Graham, and I'm delighted to make your acquaintance.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"\\n------ Sample {i + 1} ----\")\n",
    "    print(sample[\"text\"])\n",
    "    if i > 2:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9acb90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset manuallyâ€¦\n",
      "Processing example 0/484\n",
      "Processing example 50/484\n",
      "Processing example 100/484\n",
      "Processing example 150/484\n",
      "Processing example 200/484\n",
      "Processing example 250/484\n",
      "Processing example 300/484\n",
      "Processing example 350/484\n",
      "Processing example 400/484\n",
      "Processing example 450/484\n",
      "Processed dataset size: 484\n",
      "Sample row: {'input_ids': [151644, 872, 198, 3838, 374, 697, 829, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 44978, 311, 3367, 498, 0, 3017, 829, 374, 6898, 25124, 11, 323, 358, 2776, 33972, 311, 1281, 697, 81307, 13, 151645, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [151644, 872, 198, 3838, 374, 697, 829, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 44978, 311, 3367, 498, 0, 3017, 829, 374, 6898, 25124, 11, 323, 358, 2776, 33972, 311, 1281, 697, 81307, 13, 151645, 198]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --------- Windows-safe settings ---------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "def preprocess_dataset_manually(dataset, tokenizer, max_length=2048):\n",
    "    \"\"\"\n",
    "    Convert each row's text into token IDs, attention masks and labels\n",
    "    (labels = input_ids for causal LM).\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing dataset manuallyâ€¦\")\n",
    "    processed = []\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing example {i}/{len(dataset)}\")\n",
    "\n",
    "        text = example[\"text\"]\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        processed.append({\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze().tolist(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze().tolist(),\n",
    "            \"labels\": enc[\"input_ids\"].squeeze().tolist()\n",
    "        })\n",
    "\n",
    "    return Dataset.from_list(processed)\n",
    "\n",
    "# ---- run the pre-tokeniser on *your* dataset ----\n",
    "processed_dataset = preprocess_dataset_manually(dataset, tokenizer)\n",
    "print(f\"Processed dataset size: {len(processed_dataset)}\")\n",
    "print(\"Sample row:\", processed_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f0eef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[151644, 872, 198, 3838, 594, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 594, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 3838, 374, 697, 829, 30, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>[151644, 872, 198, 22043, 697, 4008, 315, 4338...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 697, 4008, 315, 4338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6407, 304, 3213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6407, 304, 3213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6783, 15379, 31...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 279, 6783, 15379, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>[151644, 872, 198, 28715, 389, 697, 1467, 11, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 28715, 389, 697, 1467, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>[151644, 872, 198, 22043, 429, 498, 3003, 9733...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[151644, 872, 198, 22043, 429, 498, 3003, 9733...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "1    [151644, 872, 198, 3838, 594, 697, 829, 30, 15...   \n",
       "2    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "3    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "4    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...   \n",
       "..                                                 ...   \n",
       "479  [151644, 872, 198, 22043, 697, 4008, 315, 4338...   \n",
       "480  [151644, 872, 198, 22043, 279, 6407, 304, 3213...   \n",
       "481  [151644, 872, 198, 22043, 279, 6783, 15379, 31...   \n",
       "482  [151644, 872, 198, 28715, 389, 697, 1467, 11, ...   \n",
       "483  [151644, 872, 198, 22043, 429, 498, 3003, 9733...   \n",
       "\n",
       "                                        attention_mask  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "479  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "480  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "481  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "482  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "483  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                labels  \n",
       "0    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "1    [151644, 872, 198, 3838, 594, 697, 829, 30, 15...  \n",
       "2    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "3    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "4    [151644, 872, 198, 3838, 374, 697, 829, 30, 15...  \n",
       "..                                                 ...  \n",
       "479  [151644, 872, 198, 22043, 697, 4008, 315, 4338...  \n",
       "480  [151644, 872, 198, 22043, 279, 6407, 304, 3213...  \n",
       "481  [151644, 872, 198, 22043, 279, 6783, 15379, 31...  \n",
       "482  [151644, 872, 198, 28715, 389, 697, 1467, 11, ...  \n",
       "483  [151644, 872, 198, 22043, 429, 498, 3003, 9733...  \n",
       "\n",
       "[484 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# if it's a single split Dataset\n",
    "df = processed_dataset.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acacefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=processed_dataset,   # pre-tokenised!\n",
    "    dataset_text_field=None,           # important: no extra mapping\n",
    "    max_seq_length=2048,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch\",           # safer on Windows than adamw_8bit\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa85e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 484 | Num Epochs = 5 | Total steps = 305\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 17,432,576 of 1,738,007,552 (1.00% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 11:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.827800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.945900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.959800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.717600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.739700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.568100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.623100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>1.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>1.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>1.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>1.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>1.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>1.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>1.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>1.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>1.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>1.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>1.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>1.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>1.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>1.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>1.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>1.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>1.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>1.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>1.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>1.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>1.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>1.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>1.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>1.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>1.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>1.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>1.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>1.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>1.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>1.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>1.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>1.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>1.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>1.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>1.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>1.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>1.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>1.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>1.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>1.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>1.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>1.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>1.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>1.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>1.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>1.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>1.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>1.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>1.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.310800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c7767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How to do a startup?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\"Listen, if you want to do a startup, you gotta be willing to put in the work. I mean, it's not like it's gonna be easy, and it's gonna take a lot of energy and focus. But if you're really passionate about what you're doing, the drive will get you through it. And don't even get me started on the sleep deprivation... it's going to be hell. But if you're gonna do it, do it like a true entrepreneur. Be relentless, be ruthless, and be willing to kill your darlings just to get to the top. And, of course, don't forget to seek advice from other founders, because nobody's gonna take the shortcut for you. So, if you're serious about doing a startup, don't wait for someone to tell you how. Just get out there and make it happen.\"<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How to do a startup?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 200, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e432d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69.8M/69.8M [00:34<00:00, 2.02MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Sri1999/Qwen3-1.7B-Instruct-Paul-Graham-LORA\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\n",
    "    \"Sri1999/Qwen3-1.7B-Instruct-Paul-Graham-LORA\",  \n",
    "    tokenizer, \n",
    "    token = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acdf1a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:56<00:00, 118.49s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to: D:\\ascii_art_completion_finetuning\\merged-qwen3-1.7b\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_id = \"Qwen/Qwen3-1.7B\"\n",
    "lora_ckpt = r\"D:\\ascii_art_completion_finetuning\\outputs\\checkpoint-305\"\n",
    "out_dir  = r\"D:\\ascii_art_completion_finetuning\\merged-qwen3-1.7b\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_id, trust_remote_code=True,\n",
    "    torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_ckpt)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(out_dir, safe_serialization=True)\n",
    "tok.save_pretrained(out_dir)\n",
    "\n",
    "print(\"Merged model saved to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e48e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python .\\convert_hf_to_gguf.py ..\\merged-qwen3-1.7b --outfile ..\\qwen3-paulgraham-f16.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_file\n",
    "\n",
    "repo_id = \"Sri1999/Qwen3-0.6B-ascii-cats-lora-GGUF\"\n",
    "\n",
    "# 1ï¸âƒ£ Create the repo if it doesn't exist\n",
    "create_repo(repo_id, repo_type=\"model\", private=False, exist_ok=True)\n",
    "\n",
    "# 2ï¸âƒ£ Upload the GGUF file\n",
    "upload_file(\n",
    "    path_or_fileobj=r\"D:\\ascii_art_completion_finetuning\\qwen3-ascii-f16.gguf\",\n",
    "    path_in_repo=\"qwen3-ascii-f16.gguf\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Uploaded to https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b2d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unsloth_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "qwen3-paulgraham-f16.gguf:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.22G/3.45G [27:26<195:37:57, 3.16kB/s]'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host')), '(Request ID: 9f84a2a2-fb3c-4c48-92bd-8b9a13751b48)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=2331fb0a2ceeb742bb3436ded736d908755b16ea2e3dec480435f5ec44593520&X-Amz-SignedHeaders=host&partNumber=77&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "qwen3-paulgraham-f16.gguf:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.88G/3.45G [31:59<06:35, 3.95MB/s]    '(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 83fd6922-0ad0-402e-9631-8b84c182c3b5)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096E26950>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 188e66c8-543c-4a5c-a253-2bdc48a9c29d)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096E986D0>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 900eeaec-28df-4d24-b57f-3a5d905bedcc)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096D26F10>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: dbb93296-d169-4596-9e81-d1945951f5b0)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002A096E0A690>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 563598a9-6045-47c2-8841-c7c013a4e1c6)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=e43f7f5790b8f98da4a8b0927fde5b5a83d2a21802905543fba704df75c5a70d&X-Amz-SignedHeaders=host&partNumber=117&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 8s [Retry 5/5].\n",
      "qwen3-paulgraham-f16.gguf:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2.19G/3.45G [35:16<04:35, 4.57MB/s]  '(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host')), '(Request ID: 77a07bff-1c52-479a-bae5-3ad77321ca5c)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/99/d6/99d6498ab0e93df1b4efac867769971ad2c0468054e45c6d0206f3e4267df6f0/f9167fd1cb841ee876ec457795d67f30798916053cb9ee7e5efb86e0fe99bce8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T020927Z&X-Amz-Expires=86400&X-Amz-Signature=5cf3716c58945efdd437066a0e521bef266c01040803bc0f097ab1e478045bd8&X-Amz-SignedHeaders=host&partNumber=135&uploadId=pxUTY2XwO_4IZKQ56eL2RHkt8HNFTKH4S4OZWZV2sWZk2EcNhHcw8aKoaBkgrjyq1bcbsyfoiO1J6BN7.DEZu9cFpQZpjd2C7zOPJNCkO7oqm7T4rM3O79Ctao2VweOS&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "qwen3-paulgraham-f16.gguf: 3.49GB [44:52, 1.30MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Uploaded to https://huggingface.co/Sri1999/Qwen3-1.7b-paulgraham-GGUF\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_file\n",
    "\n",
    "repo_id = \"Sri1999/Qwen3-1.7b-paulgraham-GGUF\"\n",
    "\n",
    "# 1ï¸âƒ£ Create the repo if it doesn't exist\n",
    "create_repo(repo_id, repo_type=\"model\", private=False, exist_ok=True)\n",
    "\n",
    "# 2ï¸âƒ£ Upload the GGUF file\n",
    "upload_file(\n",
    "    path_or_fileobj=r\"D:\\ascii_art_completion_finetuning\\qwen3-paulgraham-f16.gguf\",\n",
    "    path_in_repo=\"qwen3-paulgraham-f16.gguf\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Uploaded to https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e72f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model once\n",
    "llm = Llama(\n",
    "    model_path=r\"D:\\ascii_art_completion_finetuning\\qwen3-ascii-f16.gguf\",\n",
    "    n_ctx=8192,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def ask_llm(question: str,\n",
    "            max_tokens: int = 256,\n",
    "            generation_config: dict | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Streams a response from the GGUF model and returns the full text.\n",
    "    \"\"\"\n",
    "    if generation_config is None:\n",
    "        generation_config = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"min_p\": 0.0,\n",
    "            \"frequency_penalty\": 0.0,\n",
    "            \"presence_penalty\": 0.0,\n",
    "            \"repeat_penalty\": 1.1,\n",
    "            \"top_k\": 40\n",
    "        }\n",
    "\n",
    "    full_text = \"\"\n",
    "    prompt = f\"User: {question}\\nAssistant:\"  # customise prompt style if needed\n",
    "\n",
    "    for chunk in llm.create_completion(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        top_p=generation_config[\"top_p\"],\n",
    "        min_p=generation_config[\"min_p\"],\n",
    "        frequency_penalty=generation_config[\"frequency_penalty\"],\n",
    "        presence_penalty=generation_config[\"presence_penalty\"],\n",
    "        repeat_penalty=generation_config[\"repeat_penalty\"],\n",
    "        top_k=generation_config[\"top_k\"],\n",
    "    ):\n",
    "        text_piece = chunk[\"choices\"][0][\"text\"]\n",
    "        print(text_piece, end=\"\", flush=True)   # live streaming to console\n",
    "        full_text += text_piece\n",
    "\n",
    "    print()  # final newline\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31528ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Well, let's start with the basics. In a regular computer, we have something like a circuit board and tiny chips that make up all the logic needed to process information. But in quantum computing, these components are different. Instead of bits or numbers, we use qubits.\n",
      "\n",
      "So a qubit is a basic unit of information. Unlike a bit, which can be 0 or 1, a qubit can be both 0 and 1 at the same time. This is because it's in superposition. A qubit stays in a state of either 0 or 1 until measured by an external device.\n",
      "\n",
      "Then there's entanglement. When two qubits are connected, they become correlated, so changing one affects the other instantly, no matter how far apart they are. This enables quantum computers to perform complex calculations faster than classical ones.\n",
      "\n",
      "These two properties (superposition and entanglement) make quantum computers very powerful for solving specific problems, like factoring large numbers or\n",
      "\n",
      "Full answer captured:\n",
      "  Well, let's start with the basics. In a regular computer, we have something like a circuit board and tiny chips that make up all the logic needed to process information. But in quantum computing, these components are different. Instead of bits or numbers, we use qubits.\n",
      "\n",
      "So a qubit is a basic unit of information. Unlike a bit, which can be 0 or 1, a qubit can be both 0 and 1 at the same time. This is because it's in superposition. A qubit stays in a state of either 0 or 1 until measured by an external device.\n",
      "\n",
      "Then there's entanglement. When two qubits are connected, they become correlated, so changing one affects the other instantly, no matter how far apart they are. This enables quantum computers to perform complex calculations faster than classical ones.\n",
      "\n",
      "These two properties (superposition and entanglement) make quantum computers very powerful for solving specific problems, like factoring large numbers or\n"
     ]
    }
   ],
   "source": [
    "answer = ask_llm(\"Explain quantum computing in simple terms.\", max_tokens=200)\n",
    "print(\"\\nFull answer captured:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb113e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
